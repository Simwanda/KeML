{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# KeML Framework for RCFST Columns under Eccentric Loading\n\nThis notebook implements a **knowledge-enhanced machine learning (KeML)** framework\nfor predicting the ultimate strength of **rectangular concrete-filled steel tube (RCFST)**\ncolumns under eccentric compression.\n\n**Before running:**\n\n1. Create a folder called `data` in the same directory as this notebook.\n2. Put your database file there, named: `rcfst_database.csv`  \n3. The CSV should contain at least these columns (you can rename in the code if needed):\n\n   - `H`, `B`, `t`, `L`, `e`, `fy`, `fc_prime`, `Nu_exp`\n   - Optionally: `N_empirical` with predictions from any design equation (e.g. Han, Naser, EC4).\n     If not present, the plastic resistance `Npl` will be used as the knowledge term.\n\nThe notebook will automatically create:\n\n- `artifacts/` for saved models\n- `outputs/figures/` for parity plots\n- `outputs/metrics/` for CSV tables of metrics\n- `outputs/predictions/` for CSV files of predictions\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import mutual_info_regression, f_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import (\n    r2_score,\n    mean_absolute_error,\n    mean_squared_error\n)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nfrom xgboost import XGBRegressor\nimport joblib\n\n# Optional (only needed if you enable tuning later)\nimport optuna\n\n# ----------------------\n# Paths and basic config\n# ----------------------\n\nPROJECT_ROOT = Path.cwd()\nDATA_DIR     = PROJECT_ROOT / \"data\"\nARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\nOUTPUTS_DIR   = PROJECT_ROOT / \"outputs\"\n\nfor p in [\n    ARTIFACTS_DIR,\n    ARTIFACTS_DIR / \"models\",\n    ARTIFACTS_DIR / \"optuna\",\n    OUTPUTS_DIR,\n    OUTPUTS_DIR / \"figures\",\n    OUTPUTS_DIR / \"metrics\",\n    OUTPUTS_DIR / \"predictions\",\n]:\n    os.makedirs(p, exist_ok=True)\n\nDB_CSV_PATH = DATA_DIR / \"rcfst_database.csv\"\n\nRANDOM_STATE = 42\nTEST_SIZE    = 0.2\n\nprint(\"Project root:\", PROJECT_ROOT)\nprint(\"Data path:\", DB_CSV_PATH)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# =======================================\n# 1. Data utilities & secondary features\n# =======================================\n\ndef steel_area_rect_hollow(H: float, B: float, t: float) -> float:\n    \"\"\"Approximate steel area of a rectangular hollow section:\n    As = H*B - (H - 2t)*(B - 2t)\n    Units must be consistent with your input.\n    \"\"\"\n    return H * B - (H - 2.0 * t) * (B - 2.0 * t)\n\ndef concrete_area_rect(H: float, B: float, t: float) -> float:\n    \"\"\"Concrete core area inside the steel tube:\n    Ac = (H - 2t)*(B - 2t)\n    \"\"\"\n    return (H - 2.0 * t) * (B - 2.0 * t)\n\ndef compute_secondary_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Compute secondary features for RCFST columns.\n\n    Assumes primary columns:\n      H, B, t, L, e, fy, fc_prime\n    \"\"\"\n    out = df.copy()\n\n    H  = out[\"H\"].values\n    B  = out[\"B\"].values\n    t  = out[\"t\"].values\n    L  = out[\"L\"].values\n    e  = out[\"e\"].values\n    fy = out[\"fy\"].values\n    fc = out[\"fc_prime\"].values\n\n    # Areas and plastic axial resistance\n    As = steel_area_rect_hollow(H, B, t)\n    Ac = concrete_area_rect(H, B, t)\n\n    Asfy = As * fy\n    Acfc = Ac * fc\n    Npl  = Asfy + Acfc  # simple plastic resistance\n\n    # Geometric slenderness-like quantities\n    lam = 2.0 * (3.0 ** 0.5) * L / H      # \u03bb = 2*sqrt(3)*L/H\n    e_over_H = e / H\n    H_over_t = H / t\n    delta = Asfy / np.maximum(Npl, 1e-8)\n    alpha = As / np.maximum(Ac, 1e-8)\n\n    # Approximate material stiffness (adjust Es, Ec if you have better values)\n    Es = 2.0e5  # MPa\n    Ec = 4700.0 * np.sqrt(fc)  # MPa, EC2-style\n\n    # 2nd moments of area as hollow rectangle (about strong axis)\n    I_steel = (B * H**3 - (B - 2.0*t) * (H - 2.0*t)**3) / 12.0\n    I_conc  = ((B - 2.0*t) * (H - 2.0*t)**3) / 12.0\n\n    EIeff_s = Es * I_steel\n    EIeff_c = Ec * I_conc\n\n    # Euler-type elastic buckling resistance (very approximate)\n    Ncr = (np.pi**2) * (EIeff_s + 0.6 * EIeff_c) / np.maximum(L**2, 1e-8)\n\n    # Non-dimensional slenderness and reduction factor chi (EC3 / EC4 style)\n    lam_bar = np.sqrt(np.maximum(Npl / np.maximum(Ncr, 1e-8), 1e-8))\n    alpha_chi = 0.21\n    phi = 0.5 * (1.0 + alpha_chi * (lam_bar - 0.2) + lam_bar**2)\n    chi = 1.0 / (phi + np.sqrt(np.maximum(phi**2 - lam_bar**2, 1e-8)))\n\n    # Attach to DataFrame\n    out[\"As\"] = As\n    out[\"Ac\"] = Ac\n    out[\"Asfy\"]   = Asfy\n    out[\"Acfc\"]   = Acfc\n    out[\"Npl\"]    = Npl\n    out[\"lambda\"] = lam\n    out[\"e_over_H\"] = e_over_H\n    out[\"H_over_t\"] = H_over_t\n    out[\"delta\"]    = delta\n    out[\"alpha\"]    = alpha\n    out[\"EIeff_s\"]  = EIeff_s\n    out[\"EIeff_c\"]  = EIeff_c\n    out[\"Ncr\"]      = Ncr\n    out[\"chi\"]      = chi\n\n    return out\n\ndef load_and_prepare_database(\n    csv_path: Optional[Path] = None,\n    use_empirical_column: bool = True,\n    empirical_col_name: str = \"N_empirical\"\n) -> Tuple[pd.DataFrame, pd.Series, pd.Series, List[str]]:\n    \"\"\"\n    Load the database CSV, compute secondary features, and return:\n      X        : feature matrix (DataFrame)\n      y        : experimental ultimate strength\n      y_emp    : empirical baseline (Han, Naser, EC4, etc.)\n      features : list of feature names used\n\n    If `use_empirical_column` is False or the column is missing,\n    the baseline is taken as `Npl`.\n    \"\"\"\n    path = csv_path or DB_CSV_PATH\n    df_raw = pd.read_csv(path)\n\n    # Compute secondary features\n    df = compute_secondary_features(df_raw)\n\n    # Experimental target\n    y = df[\"Nu_exp\"].astype(float)\n\n    # Knowledge term: empirical design equation or fallback Npl\n    if use_empirical_column and empirical_col_name in df.columns:\n        y_emp = df[empirical_col_name].astype(float)\n    else:\n        y_emp = df[\"Npl\"].astype(float)\n\n    # 15-feature example (you can customise this list)\n    primary_feats = [\"H\", \"B\", \"t\", \"L\", \"e\", \"fy\", \"fc_prime\"]\n    secondary_feats = [\n        \"e_over_H\", \"lambda\", \"Asfy\", \"Acfc\",\n        \"alpha\", \"EIeff_s\", \"EIeff_c\", \"Ncr\",\n    ]\n    feature_names = primary_feats + secondary_feats\n\n    X = df[feature_names].astype(float)\n\n    return X, y, y_emp, feature_names\n\ndef train_test_split_rcfst(\n    X: pd.DataFrame,\n    y: pd.Series,\n    y_emp: pd.Series,\n    test_size: float = TEST_SIZE,\n    random_state: int = RANDOM_STATE\n):\n    \"\"\"Train-test split, keeping the empirical predictions aligned.\"\"\"\n    X_train, X_test, y_train, y_test, y_emp_train, y_emp_test = train_test_split(\n        X, y, y_emp,\n        test_size=test_size,\n        random_state=random_state\n    )\n    return X_train, X_test, y_train, y_test, y_emp_train, y_emp_test\n\n# Quick sanity check (will fail if CSV is missing)\nif DB_CSV_PATH.exists():\n    X_demo, y_demo, y_emp_demo, feats_demo = load_and_prepare_database()\n    print(\"Loaded data with shape:\", X_demo.shape)\nelse:\n    print(\"WARNING: rcfst_database.csv not found yet. Place it under ./data\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# =======================================\n# 2. Feature scaling & combined weights\n# =======================================\n\ndef standardize_features(\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame\n) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:\n    \"\"\"Standardize features using StandardScaler fitted on training data only.\"\"\"\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train.values)\n    X_test_scaled  = scaler.transform(X_test.values)\n    return X_train_scaled, X_test_scaled, scaler\n\ndef compute_combined_feature_weights(\n    X: pd.DataFrame,\n    y: pd.Series\n) -> pd.DataFrame:\n    \"\"\"\n    Compute combined feature weights using:\n      - SVR (linear) coefficients\n      - XGBRegressor feature importances\n      - mutual_info_regression\n      - f_regression\n\n    Each set of scores is normalized and then averaged; the combined weight\n    is finally rescaled by its maximum.\n    \"\"\"\n    feature_names = X.columns.tolist()\n    X_val = X.values\n    y_val = y.values\n\n    # 1) SVR with linear kernel\n    svr = SVR(kernel=\"linear\")\n    svr.fit(X_val, y_val)\n    svr_imp = np.abs(svr.coef_).ravel()\n\n    # 2) XGBoost gains\n    xgb = XGBRegressor(\n        n_estimators=200,\n        max_depth=4,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective=\"reg:squarederror\",\n        random_state=0\n    )\n    xgb.fit(X_val, y_val)\n    xgb_imp = xgb.feature_importances_\n\n    # 3) Mutual information\n    mi = mutual_info_regression(X_val, y_val, random_state=0)\n\n    # 4) f_regression\n    f_vals, _ = f_regression(X_val, y_val)\n    f_vals = np.nan_to_num(f_vals, nan=0.0)\n\n    def normalize(v):\n        s = v.sum()\n        return v / s if s > 0 else np.zeros_like(v)\n\n    w_svr = normalize(svr_imp)\n    w_xgb = normalize(xgb_imp)\n    w_mi  = normalize(mi)\n    w_f   = normalize(f_vals)\n\n    mean_w = (w_svr + w_xgb + w_mi + w_f) / 4.0\n    cwv = mean_w / np.max(mean_w)  # combined weight value\n\n    df_w = pd.DataFrame({\n        \"feature\": feature_names,\n        \"w_SVR\": w_svr,\n        \"w_XGB\": w_xgb,\n        \"w_MI\":  w_mi,\n        \"w_F\":   w_f,\n        \"mean_w\": mean_w,\n        \"CWV\": cwv\n    }).sort_values(\"CWV\", ascending=False).reset_index(drop=True)\n\n    return df_w\n\ndef drop_highly_correlated_features(\n    X: pd.DataFrame,\n    cwv_df: pd.DataFrame,\n    corr_threshold: float = 0.75\n) -> Tuple[pd.DataFrame, List[str]]:\n    \"\"\"\n    Drop highly correlated features based on a correlation threshold,\n    keeping the more important one according to CWV.\n    \"\"\"\n    corr = X.corr().abs()\n    ordered = cwv_df[\"feature\"].tolist()\n\n    to_keep = set(ordered)\n    to_drop = set()\n\n    for i, fi in enumerate(ordered):\n        if fi in to_drop:\n            continue\n        for fj in ordered[i+1:]:\n            if fj in to_drop:\n                continue\n            if corr.loc[fi, fj] > corr_threshold:\n                wi = cwv_df.loc[cwv_df[\"feature\"] == fi, \"CWV\"].values[0]\n                wj = cwv_df.loc[cwv_df[\"feature\"] == fj, \"CWV\"].values[0]\n                if wi >= wj:\n                    to_drop.add(fj)\n                else:\n                    to_drop.add(fi)\n                    break\n\n    retained = list(to_keep - to_drop)\n    retained = [f for f in ordered if f in retained]\n\n    X_red = X[retained].copy()\n    return X_red, retained\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# =======================================\n# 3. KeML residual-learning regressor\n# =======================================\n\n@dataclass\nclass KeMLRegressor:\n    \"\"\"Knowledge-enhanced ML regressor.\n\n    Model form:\n        y = y_emp + \u03b4(x, y_emp)\n        \u03b4(x, y_emp) = f_L(x, y_emp) + f_n(x, y_emp)\n\n    where:\n      - y_emp: empirical prediction (knowledge)\n      - f_L  : linear residual (LinearRegression)\n      - f_n  : nonlinear residual (any regressor, e.g., SVR/RF/XGB/ANN)\n    \"\"\"\n    base_model: Any\n    fit_base_model: bool = True\n\n    def __post_init__(self):\n        self.lin_ = LinearRegression()\n        self._fitted = False\n\n    def _stack_features(self, X: np.ndarray, y_emp: np.ndarray) -> np.ndarray:\n        y_emp_col = np.asarray(y_emp).reshape(-1, 1)\n        return np.hstack([X, y_emp_col])\n\n    def fit(self, X: np.ndarray, y: np.ndarray, y_emp: np.ndarray):\n        X = np.asarray(X)\n        y = np.asarray(y).ravel()\n        y_emp = np.asarray(y_emp).ravel()\n\n        Z = self._stack_features(X, y_emp)\n        delta = y - y_emp\n\n        # 1) Fit linear component\n        self.lin_.fit(Z, delta)\n        delta_lin = self.lin_.predict(Z)\n\n        # 2) Fit nonlinear residual\n        delta_nl = delta - delta_lin\n        if self.fit_base_model and self.base_model is not None:\n            self.base_model.fit(Z, delta_nl)\n\n        self._fitted = True\n        return self\n\n    def predict(self, X: np.ndarray, y_emp: np.ndarray) -> np.ndarray:\n        if not self._fitted:\n            raise RuntimeError(\"KeMLRegressor must be fitted before predicting.\")\n\n        X = np.asarray(X)\n        y_emp = np.asarray(y_emp).ravel()\n        Z = self._stack_features(X, y_emp)\n\n        delta_lin = self.lin_.predict(Z)\n        if self.base_model is not None:\n            delta_nl = self.base_model.predict(Z)\n        else:\n            delta_nl = 0.0\n\n        return y_emp + delta_lin + delta_nl\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# =======================================\n# 4. Original models & Optuna tuning\n# =======================================\n\ndef build_original_models() -> Dict[str, Any]:\n    \"\"\"Return a dictionary of original ML models (no knowledge term).\"\"\"\n    models = {\n        \"KNN\": KNeighborsRegressor(\n            n_neighbors=3,\n            weights=\"distance\"\n        ),\n        \"SVR\": SVR(\n            kernel=\"rbf\",\n            C=100.0,\n            gamma=\"scale\",\n            epsilon=0.1\n        ),\n        \"DT\": DecisionTreeRegressor(\n            max_depth=20,\n            random_state=0\n        ),\n        \"ANN\": MLPRegressor(\n            hidden_layer_sizes=(80, 80),\n            activation=\"relu\",\n            solver=\"adam\",\n            learning_rate_init=0.01,\n            max_iter=500,\n            random_state=0\n        ),\n        \"RF\": RandomForestRegressor(\n            n_estimators=200,\n            max_depth=30,\n            random_state=0,\n            n_jobs=-1\n        ),\n        \"XGB\": XGBRegressor(\n            n_estimators=300,\n            max_depth=5,\n            learning_rate=0.05,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            objective=\"reg:squarederror\",\n            random_state=0\n        )\n    }\n    return models\n\n# -------------------\n# Optuna for XGB (optional)\n# -------------------\n\ndef objective_xgb(trial: optuna.Trial, X, y) -> float:\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 400),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n    }\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n    rmses = []\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n\n        model = XGBRegressor(\n            objective=\"reg:squarederror\",\n            random_state=0,\n            **params\n        )\n        model.fit(X_tr, y_tr)\n        y_pred = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n        rmses.append(rmse)\n\n    return float(np.mean(rmses))\n\ndef tune_xgb_with_optuna(\n    X,\n    y,\n    n_trials: int = 50,\n    study_name: str = \"xgb_keml_tuning\"\n) -> XGBRegressor:\n    \"\"\"Run Optuna tuning for XGBRegressor and return the best-fitted model.\"\"\"\n    study = optuna.create_study(\n        study_name=study_name,\n        direction=\"minimize\"\n    )\n    study.optimize(lambda trial: objective_xgb(trial, X, y), n_trials=n_trials)\n\n    best_params = study.best_params\n    best_model = XGBRegressor(\n        objective=\"reg:squarederror\",\n        random_state=0,\n        **best_params\n    )\n    best_model.fit(X, y)\n\n    # Save study and model\n    optuna_path = ARTIFACTS_DIR / \"optuna\" / f\"{study_name}_study.pkl\"\n    model_path  = ARTIFACTS_DIR / \"models\"  / f\"{study_name}_best_xgb.joblib\"\n    joblib.dump(study, optuna_path)\n    joblib.dump(best_model, model_path)\n\n    print(\"Best XGB params:\", best_params)\n    return best_model\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# =======================================\n# 5. Evaluation helpers (metrics + plots)\n# =======================================\n\ndef regression_metrics(y_true, y_pred) -> Dict[str, float]:\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n\n    r2 = r2_score(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mape = np.mean(np.abs((y_pred - y_true) / y_true)) * 100.0\n\n    return {\n        \"R2\": float(r2),\n        \"MAE\": float(mae),\n        \"RMSE\": float(rmse),\n        \"MAPE\": float(mape),\n    }\n\ndef save_metrics_table(\n    metrics_dict: Dict[str, Dict[str, float]],\n    filename: str\n):\n    df = pd.DataFrame(metrics_dict).T\n    path = OUTPUTS_DIR / \"metrics\" / filename\n    df.to_csv(path, index=True)\n    print(\"Saved metrics to:\", path)\n    return path\n\ndef parity_plot(\n    y_true,\n    y_pred,\n    title: str,\n    fname: str,\n    split: str = \"\"\n):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    fig, ax = plt.subplots()\n    ax.scatter(y_true, y_pred, s=20, alpha=0.7)\n\n    # y = x and \u00b120% bounds\n    min_val = min(y_true.min(), y_pred.min())\n    max_val = max(y_true.max(), y_pred.max())\n    x_line = np.linspace(min_val, max_val, 200)\n    ax.plot(x_line, x_line, \"k-\", label=\"y = x\")\n    ax.plot(x_line, 0.8 * x_line, \"b--\", label=\"\u00b120% bounds\")\n    ax.plot(x_line, 1.2 * x_line, \"b--\")\n\n    ax.set_xlabel(\"Experimental Nu\")\n    ax.set_ylabel(\"Predicted Nu\")\n    full_title = title if not split else f\"{title} ({split})\"\n    ax.set_title(full_title)\n    ax.legend()\n\n    fig_path = OUTPUTS_DIR / \"figures\" / fname\n    fig.tight_layout()\n    fig.savefig(fig_path, dpi=300)\n    plt.close(fig)\n    print(\"Saved parity plot to:\", fig_path)\n    return fig_path\n\ndef save_predictions(\n    specimen_ids,\n    y_true,\n    y_pred,\n    model_name: str,\n    split: str\n):\n    df = pd.DataFrame({\n        \"specimen_id\": specimen_ids,\n        \"Nu_exp\": np.asarray(y_true).ravel(),\n        f\"Nu_pred_{model_name}_{split}\": np.asarray(y_pred).ravel(),\n    })\n    path = OUTPUTS_DIR / \"predictions\" / f\"pred_{model_name}_{split}.csv\"\n    df.to_csv(path, index=False)\n    print(\"Saved predictions to:\", path)\n    return path\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# =======================================\n# 6. Main pipeline: ORIGINAL vs KeML\n# =======================================\n\n# 6.1 Load database\nif not DB_CSV_PATH.exists():\n    raise FileNotFoundError(\n        f\"Database CSV not found at {DB_CSV_PATH}. Please create 'data/rcfst_database.csv' first.\"\n    )\n\nX, y, y_emp, feature_names = load_and_prepare_database()\nprint(\"Feature names:\", feature_names)\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)\nprint(\"y_emp shape:\", y_emp.shape)\n\n# 6.2 Combined weights + optional correlation-based feature pruning\nweights_df = compute_combined_feature_weights(X, y)\ndisplay(weights_df)\n\nX_reduced, retained_features = drop_highly_correlated_features(X, weights_df, corr_threshold=0.75)\nprint(\"Retained features after correlation filtering:\", retained_features)\nprint(\"X_reduced shape:\", X_reduced.shape)\n\n# 6.3 Train-test split (use reduced features)\nX_train, X_test, y_train, y_test, y_emp_train, y_emp_test = train_test_split_rcfst(\n    X_reduced, y, y_emp\n)\n\nspecimen_ids_train = X_train.index.to_numpy()\nspecimen_ids_test  = X_test.index.to_numpy()\n\n# 6.4 Standardize features\nX_train_scaled, X_test_scaled, scaler = standardize_features(X_train, X_test)\nscaler_path = ARTIFACTS_DIR / \"models\" / \"scaler.joblib\"\njoblib.dump(scaler, scaler_path)\nprint(\"Saved scaler to:\", scaler_path)\n\n# 6.5 Original ML models (no knowledge)\norig_models = build_original_models()\norig_metrics_train = {}\norig_metrics_test  = {}\n\nfor name, model in orig_models.items():\n    print(f\"\\nTraining ORIGINAL model: {name}\")\n    model.fit(X_train_scaled, y_train)\n\n    y_pred_tr = model.predict(X_train_scaled)\n    y_pred_te = model.predict(X_test_scaled)\n\n    m_tr = regression_metrics(y_train, y_pred_tr)\n    m_te = regression_metrics(y_test,  y_pred_te)\n\n    orig_metrics_train[f\"{name}_orig_train\"] = m_tr\n    orig_metrics_test[f\"{name}_orig_test\"]   = m_te\n\n    # Save model\n    model_path = ARTIFACTS_DIR / \"models\" / f\"{name}_orig.joblib\"\n    joblib.dump(model, model_path)\n\n    # Plots + predictions\n    parity_plot(y_train, y_pred_tr, f\"{name} ORIGINAL\", f\"parity_{name}_orig_train.png\", \"train\")\n    parity_plot(y_test,  y_pred_te, f\"{name} ORIGINAL\", f\"parity_{name}_orig_test.png\",  \"test\")\n\n    save_predictions(specimen_ids_train, y_train, y_pred_tr, f\"{name}_orig\", \"train\")\n    save_predictions(specimen_ids_test,  y_test,  y_pred_te,  f\"{name}_orig\", \"test\")\n\n\nsave_metrics_table(orig_metrics_train, \"metrics_original_train.csv\")\nsave_metrics_table(orig_metrics_test,  \"metrics_original_test.csv\")\n\n# 6.6 (OPTIONAL) Hyperparameter tuning for XGB\ndo_tuning = False  # set True to run Optuna tuning (can take time)\n\nif do_tuning:\n    best_xgb = tune_xgb_with_optuna(\n        X_train_scaled,\n        y_train.values,\n        n_trials=50,\n        study_name=\"xgb_rcfst\"\n    )\n    orig_models[\"XGB\"] = best_xgb\n\n# 6.7 KeML models (knowledge-enhanced)\nkeml_models = {}\nkeml_metrics_train = {}\nkeml_metrics_test  = {}\n\nfor name, base_model in orig_models.items():\n    print(f\"\\nTraining KeML model (base = {name})\")\n    keml = KeMLRegressor(base_model=base_model)\n    keml.fit(X_train_scaled, y_train.values, y_emp_train.values)\n\n    y_pred_tr_k = keml.predict(X_train_scaled, y_emp_train.values)\n    y_pred_te_k = keml.predict(X_test_scaled,  y_emp_test.values)\n\n    m_tr_k = regression_metrics(y_train, y_pred_tr_k)\n    m_te_k = regression_metrics(y_test,  y_pred_te_k)\n\n    keml_metrics_train[f\"{name}_KeML_train\"] = m_tr_k\n    keml_metrics_test[f\"{name}_KeML_test\"]   = m_te_k\n\n    keml_models[name] = keml\n\n    model_path = ARTIFACTS_DIR / \"models\" / f\"{name}_KeML.joblib\"\n    joblib.dump(keml, model_path)\n\n    parity_plot(y_train, y_pred_tr_k, f\"{name} KeML\", f\"parity_{name}_keml_train.png\", \"train\")\n    parity_plot(y_test,  y_pred_te_k, f\"{name} KeML\", f\"parity_{name}_keml_test.png\",  \"test\")\n\n    save_predictions(specimen_ids_train, y_train, y_pred_tr_k, f\"{name}_KeML\", \"train\")\n    save_predictions(specimen_ids_test,  y_test,  y_pred_te_k,  f\"{name}_KeML\", \"test\")\n\n\nsave_metrics_table(keml_metrics_train, \"metrics_keml_train.csv\")\nsave_metrics_table(keml_metrics_test,  \"metrics_keml_test.csv\")\n\n\n# 6.8 Combined comparison table (TEST only)\ndf_orig_test = pd.DataFrame(orig_metrics_test).T\ndf_keml_test = pd.DataFrame(keml_metrics_test).T\n\ndf_orig_test[\"model_type\"] = \"original\"\ndf_keml_test[\"model_type\"] = \"KeML\"\n\ndf_all = pd.concat([df_orig_test, df_keml_test], axis=0)\ndisplay(df_all)\n\ncombined_path = OUTPUTS_DIR / \"metrics\" / \"metrics_combined_test.csv\"\ndf_all.to_csv(combined_path, index=True)\nprint(\"Saved combined test metrics to:\", combined_path)\n",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}